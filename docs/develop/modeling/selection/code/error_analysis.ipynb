{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qPBlJDHwucz"
      },
      "source": [
        "Análisis de errores en conjunto de validación\n",
        "============================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM02vWO4wuc5"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "El análisis de errores es el proceso para identificar, observar y diagnosticar predicciones erróneas de un modelo de aprendizaje automático, ayudandonos a comprender las áreas con fortalezas o debilidades de un modelo. Cuando decimos que \"la precisión del modelo es del 90%\", puede que no sea uniforme en todos los subgrupos de datos o puede haber algunas condiciones en los datos de entrada en las que el modelo falla más. Por lo tanto, es importante someter las métricas a una revisión más profunda para poder mejorarlo.\n",
        "\n",
        "En este ejemplo veremos como utilizar la herramienta de Error Analysis provista en el Responsable AI Toolbox. Para mas detalles sobre esta herramienta visite: https://github.com/microsoft/responsible-ai-toolbox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjoSE6qawuc6"
      },
      "source": [
        "## Utilizando el análisis de errores en el problema censo de la UCI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oEclTmhwuc7"
      },
      "source": [
        "> Nota: Este ejemplo fué adaptado de https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/individual-dashboards/erroranalysis-dashboard/erroranalysis-interpretability-dashboard-census.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWmaFc37wuc8"
      },
      "source": [
        "### Instalación\n",
        "\n",
        "Utilizaremos las librerías `interpret-community`, `raiwidgets` y `error-analysis`. La instalación de estos paquetes requiere de el compilador g++. Este paso no es necesario en Google Colab debido a que ya se encuentra instalado. Tampoco es necesario en sistemas operativos Windows. Si no lo tiene instalado, puede hacerlo desde:\n",
        "\n",
        "```\n",
        "apt install g++\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5m-yUvIwuc9"
      },
      "source": [
        "Para ejecutar este ejemplo, necesitaremos instalar las librerias `interpret-community`, `raiwidgets` y `error-analysis` y `lightgbm`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRlKo48owuc_"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/E72102/master/docs/develop/modeling/selection/code/error_analysis.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r error_analysis.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCYoWY5twudC"
      },
      "source": [
        "> **IMPORTANTE:** Reinicie el kernel desde el menu `Runtime` (Google Colab) o `Kernel` (Jupyter) luego de realizar la instalación. Es posible que se mencionen errores de resolución de paquetes durante la instalación. Puede ignorarlos. Si desea instalar las librerias manualmente, por favor asegurese de utilizar las siguientes versiones: \n",
        "    - interpret-community==0.19\n",
        "    - raiwidgets==0.9.4\n",
        "    - scikit-learn==1.0.1\n",
        "    - erroranalysis==0.1.31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyWscZCQwudE"
      },
      "source": [
        "### Sobre el conjunto de datos del censo UCI\n",
        "\n",
        "El conjunto de datos del censo de la UCI es un conjunto de datos en el que cada registro representa a una persona. Cada registro contiene 14 columnas que describen a una una sola persona, de la base de datos del censo de Estados Unidos de 1994. Esto incluye información como la edad, el estado civil y el nivel educativo. La tarea es determinar si una persona tiene un ingreso alto (definido como ganar más de $50 mil al año). Esta tarea, dado el tipo de datos que utiliza, se usa a menudo en el estudio de equidad, en parte debido a los atributos comprensibles del conjunto de datos, incluidos algunos que contienen tipos sensibles como la edad y el género, y en parte también porque comprende una tarea claramente del mundo real."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnHZuGLSwudG"
      },
      "source": [
        "Descargamos el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcnArrfQwudH"
      },
      "outputs": [],
      "source": [
        "!wget https://santiagxf.blob.core.windows.net/public/datasets/uci_census.zip \\\n",
        "    --quiet --no-clobber\n",
        "!mkdir -p datasets/uci_census\n",
        "!unzip -qq uci_census.zip -d datasets/uci_census"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZO-pj3WwudI"
      },
      "source": [
        "Lo importamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sn4kODTMwudI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv('datasets/uci_census/data/adult-train.csv')\n",
        "test = pd.read_csv('datasets/uci_census/data/adult-test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMG5UhjewudK"
      },
      "source": [
        "### Entrenando un modelo para explorar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdNPtv-KwudK"
      },
      "source": [
        "Preparando nuestros conjuntos de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EhLsyUphwudL"
      },
      "outputs": [],
      "source": [
        "X_train = train.drop(['income'], axis=1)\n",
        "y_train = train['income'].to_numpy()\n",
        "X_test = test.drop(['income'], axis=1)\n",
        "y_test = test['income'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oZiFns7SwudL"
      },
      "outputs": [],
      "source": [
        "classes = train['income'].unique().tolist()\n",
        "features = X_train.columns.values.tolist()\n",
        "categorical_features = X_train.dtypes[X_train.dtypes == 'object'].index.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzsWIV1wudM"
      },
      "source": [
        "Realizaremos un pequeño preprocesamiento antes de entrenar el modelo:\n",
        "\n",
        "- Imputaremos los valores faltantes de las caracteristicas numéricas con la media\n",
        "- Imputaremos los valores faltantes de las caracteristicas categóricas con el valor `?`\n",
        "- Escalaremos los valores numericos utilizando un `StandardScaler`\n",
        "- Codificaremos las variables categóricas utilizando `OneHotEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "374RsZu0wudM"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "def prepare(X: pd.DataFrame) -> Tuple[np.ndarray, sklearn.compose.ColumnTransformer]:\n",
        "    pipe_cfg = {\n",
        "        'num_cols': X.dtypes[X.dtypes == 'int64'].index.values.tolist(),\n",
        "        'cat_cols': X.dtypes[X.dtypes == 'object'].index.values.tolist(),\n",
        "    }\n",
        "    \n",
        "    num_pipe = Pipeline([\n",
        "        ('num_imputer', SimpleImputer(strategy='median')),\n",
        "        ('num_scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    cat_pipe = Pipeline([\n",
        "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
        "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "    ])\n",
        "    \n",
        "    transformations = ColumnTransformer([\n",
        "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
        "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
        "    ])\n",
        "    X = transformations.fit_transform(X)\n",
        "    \n",
        "    return X, transformations\n",
        "\n",
        "\n",
        "X_train_transformed, transformations = prepare(X_train)\n",
        "X_test_transformed = transformations.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81atQ7WUwudN"
      },
      "source": [
        "Entrenamos un modelo basado en `lightgbm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "u2MVdIhKwudN"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "clf = LGBMClassifier(n_estimators=5)\n",
        "model = clf.fit(X_train_transformed, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F62qFkc2wudO"
      },
      "source": [
        "Ejecutamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZLqpnS_rwudO"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos revisar la performance del modelo:"
      ],
      "metadata": {
        "id": "Lwr57cbqJRuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMNf-9RkJTme",
        "outputId": "8bd9485a-b62e-47a6-d56d-55727ef51e19"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.81      1.00      0.90     12435\n",
            "        >50K       0.98      0.25      0.40      3846\n",
            "\n",
            "    accuracy                           0.82     16281\n",
            "   macro avg       0.90      0.63      0.65     16281\n",
            "weighted avg       0.85      0.82      0.78     16281\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weAeZyDcwudO"
      },
      "source": [
        "### Análisis de errores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjxkgFAFwudP"
      },
      "source": [
        "El análisis de errores nos permite explorar como se distributye el error de las predicciones de nuestro modelo. Una de las ventajas mas interesantes de esta herramienta es que es agnostica del modelo, es decir, que lo podemos aplicar para cualquier tipo de modelo ya que solamente necesitamos proveer los conjuntos de datos de evaluación con las predicciones que realizó el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Opcionalmente, podemos aumentar la cantidad de muestras en el conjunto de datos de validación utilizando la técnica de oversampling. Esto nos permite que la herramienta de análisis de errores tenga mas instancias para trabajar y por ende generar más opciones para visualización.\n",
        ">\n",
        "> ```python\n",
        "> test = test.sample(1000, replace=True)\n",
        "> ```\n",
        ">\n",
        "> En tal caso, recuerde ejecutar las prediciones sobre el conjunto de datos con *oversampling*:\n",
        ">\n",
        "> ```python\n",
        "> X_test = test.drop(['income'], axis=1)\n",
        "> y_test = test['income'].to_numpy()\n",
        "> X_test_transformed = transformations.transform(X_test)\n",
        "> predictions = model.predict(X_test_transformed)\n",
        "> ```"
      ],
      "metadata": {
        "id": "NsH3LDqpJtyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para abrir la herramienta, debemos construir un `ErrorAnalysisDashbord`:"
      ],
      "metadata": {
        "id": "n9diai9wKWBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ2VV6kVwudP"
      },
      "outputs": [],
      "source": [
        "from raiwidgets import ErrorAnalysisDashboard\n",
        "from interpret_community.common.constants import ModelTask\n",
        "\n",
        "ErrorAnalysisDashboard(dataset=X_test,\n",
        "                       true_y=y_test, \n",
        "                       categorical_features=categorical_features,\n",
        "                       features=features, \n",
        "                       pred_y=predictions, \n",
        "                       model_task=ModelTask.Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8hkuo7swudP"
      },
      "source": [
        "*Notas:*\n",
        " - *`dataset` es el conjunto de datos de evaluación, sin preprocesamiento y sin la variable objetivo.*\n",
        " - *`true_y` es el valor verdadero (ground-truth) de la variable a predecir*\n",
        " - *`pred_y` es el valor de las predicciones del modelo que estamos evaluando*.\n",
        " - *`categorical_features`: son los nombres de las columnas que tienen variables de tipo categorica.*\n",
        " - *`features` son los los nombres de todas las columnas que utiliza el modelo, categoricas y numeras incluidas.*\n",
        " - *`model_task` es el tipo de modelo que construirmos, donde los valores possibles son `ModelTask.Classification` o `ModelTask.Regression`.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> __¿Ve el tablero en blanco?__: Si ejecuta en Google Colab, asegurese de permitir conexiones sobre http (contenido mixto). \n",
        "> \n",
        "> \n",
        "> ![](https://user-images.githubusercontent.com/15119906/110703410-091f9480-81f4-11eb-9858-cf45a9da71cf.png)\n",
        ">\n",
        "> ![](https://user-images.githubusercontent.com/15119906/110703555-39ffc980-81f4-11eb-88db-19abb0caf2ce.png)"
      ],
      "metadata": {
        "id": "e35stQM12U-Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfmfZLNowudP"
      },
      "source": [
        "Una vez ejecutado el comando anterior, debería poder ver la herramienta de exloración de errores:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/santiagxf/E72102/main/docs/develop/_images/ea_treemap.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ppwKPrwudQ"
      },
      "source": [
        "#### Interpretación\n",
        "\n",
        "Podemos utilizar este gráfico para explorar la forma en que el modelo comente los errores. Para encontrar patrones, podemos ocmenzar buscando aquellos nodos en el arbol que tienen un color rojo más fuerte, lo que indica que esa combinación de atributos tiene un error alto al clasificarlos. El nivel de llenado del nodo indica que tan representativa es esa combinación de atributos en el conjunto de datos completo\n",
        "\n",
        "Esto quiere decir que si nos focalizamos en aquellos nodos con color más oscuro y nivel más alto, estamos atacando aquellas áreas donde tenemos más chances de mejorar la performance del modelo. Por ejemplo, en la imagen anterior vemos que cuando la relación es `Husband` o `Wife` y la cantidad de años de educación es mayor a 11.5 pero el capital es menor a $ 5035.50, estas instancias tienen una taza de error del 65% y representan el 35% de todos los errores que comete el modelo.\n",
        "\n",
        "Deberiamos investigar porque nuestro modelo no puede mapear a este tipo de instancias correctamente. Quizás haya un probema en la calidad de datos, una mala recolección, o quizás las características no fueron preprocesadas correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-2m9ga2wudQ"
      },
      "source": [
        "#### Instancias con dificultades\n",
        "\n",
        "Una característica interesante de esta libreria es la capacidad de generar mapas de calor con aquellas combinaciones de atributos donde nuestro modelo tiene problemas. Esto nos permite ver rapidamente donde el modelo tiene inconvenientes en predecir correctamente y desde allí, analizár si el modelo es aceptable cometiendo estos errores o no:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/santiagxf/E72102/main/docs/develop/_images/ea_heatmap.png\" width=\"600\" />\n",
        "\n",
        "En el ejemplo más arriba estamos comparando los predictores `relationship` y `education-num`. Como vemos, el modelo tiene grandes problemas con aquellas personas de más de 14 años de educación y que son mujeres casadas. Solo 1 persona fué clasificada correctamente representando una taza de error del 94%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT-GIKb9wudQ"
      },
      "source": [
        "### Explicaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A3XRTjEwudQ"
      },
      "source": [
        "Las explicaciones del modelo nos pueden ser útiles a la hora de explorar la importancia de cada uno de los atributos y como son utilizados por el modelo. Para generar las explicaciones del modelo, deberemos constuir un pipeline donde tengamos el preprocesamiento y el modelo propiamente dicho en un mismo objeto ya que las técnicas de explicaciones contemplan tanto las instancias de preprocesamiento como de modelado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zgRDRwEwudQ"
      },
      "source": [
        "> **IMPORTANTE:** Note que esta técnica requiere proveer el modelo original que genera la predicciones, y por lo tanto no podrá utilizarlo en escenarios donde el modelo fué entrenado en otra herramienta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMAE_OZBwudR"
      },
      "outputs": [],
      "source": [
        "model_pipeline = Pipeline(steps=[('preprocessing', transformations),\n",
        "                                 ('model', model)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTbdBbqIwudR"
      },
      "source": [
        "Configuramos un objeto para general las explicaciones del modelo basado en el conjunto de datos en el que se entreno:\n",
        "\n",
        "> **IMPORTANTE:** Note que esta técnica require la creación de un modelo que sea interpretable. Es decir, en lugar de realizar el análisis en el modelo original (el cual podría tener una complejidad arbitraria), el análsis se hace sobre un modelo que pueda ser facilmente interpretable. Para generar este segundo modelo, se utiliza la técnica de Global Model Surrogate la cual consiste en entrenar un modelo **alumno** que trata de **imitar** al modelo **profesor**. Esta técnica claramente no es exacta y no tenemos ninguna garantía de que los errores que comete el modelo **alumno** son los mismos que los que comete el alumno **profesor**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTb_QcznwudR"
      },
      "outputs": [],
      "source": [
        "from interpret_community.common.constants import ShapValuesOutput, ModelTask\n",
        "from interpret.ext.blackbox import MimicExplainer\n",
        "from interpret.ext.glassbox import LGBMExplainableModel\n",
        "\n",
        "\n",
        "explainer = MimicExplainer(model=model,\n",
        "                           initialization_examples=X_train,\n",
        "                           explainable_model=LGBMExplainableModel,\n",
        "                           augment_data=True, \n",
        "                           max_num_of_augmentations=10,\n",
        "                           features=features, \n",
        "                           classes=classes, \n",
        "                           model_task=ModelTask.Classification,\n",
        "                           transformations=transformations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG_fVjkpwudS"
      },
      "source": [
        "Generamos las explicaciones del modelo en nuestro conjunto de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmOAH_VjwudT"
      },
      "outputs": [],
      "source": [
        "global_explanation = explainer.explain_global(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS13UFTFwudT"
      },
      "source": [
        "Mostramos el tablero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtl7ajPkwudT"
      },
      "outputs": [],
      "source": [
        "from raiwidgets import ErrorAnalysisDashboard\n",
        "\n",
        "ErrorAnalysisDashboard(global_explanation, model_pipeline,\n",
        "                       dataset=X_val,\n",
        "                       true_y_dataset=y_val,\n",
        "                       true_y=y_test,\n",
        "                       categorical_features=categorical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TzcDdGqwudT"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/santiagxf/E72102/main/docs/develop/_images/ea_explanations.png\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eB3RiDlwudU"
      },
      "source": [
        "### Feature Impotance\n",
        "\n",
        "En la parte superior del gráfico vemos la importancia de cada uno de los predictores que se utilizaron en el modelo. En este caso, nos indica que `marital-status` y `education` son dos de los predictores más importantes para nuestro modelo. El concepto de aque de \"importancia\" esta atado a que, cuando estos valores cambian, la performance del modelo decrece significativamente.\n",
        "\n",
        "### Dependency plots\n",
        "\n",
        "En la parte inferior vemos como varia la importancia del predictor `ocupation` dependiendo de cada uno de todos los varios que obtiene. Este tipo de gráficos se llaman \"Dependency plots\" y nos permiten ver como la importancia de la clase que queremos predecir cambia a medida que cambian los valores de los predictores.\n",
        "\n",
        "Por ejemplo, notemos como la importancia de la variable `occupation` es muy baja cuando  cuando el valor es `priv-house-serv`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (E72.1.02)",
      "language": "python",
      "name": "e72102"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c0c26a04c01997af4d3a54c44ba2029caf4208eaf3de13f3aa81bddca06af044"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}