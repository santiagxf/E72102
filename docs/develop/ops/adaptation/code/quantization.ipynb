{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition warning\">PRECAUCI칍N 游땸: El tema presentado en esta secci칩n est치 clasificado como avanzado. El entendimiento de este contenido es totalmente opcional.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization es una t칠cnica que consiste en bajar la precisi칩n de punto flotante utilizada por nuestro modelo para que tenga requerimientos menores de memoria cuando es deplegado en producci칩n, aunque manteniendo mayoritariamente la precisi칩n que el modelo obtuvo cuando se entren칩 con precisi칩n mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivaci칩n\n",
    "----------\n",
    "\n",
    "En los ultimos tiempos - impulsados por los avances en las t칠cnicas de *deep learning* - los modelos de aprendizaje autom치tico con los que trabajamos han adquirido un tama침o considerable. Por ejemplo, modelos de NLP como RoBERTA tienen aproximadamente 125 millones de parametros. Si nos ponemos a pensar que cada par치metros termina siendo un n칰mero de punto flotante almacenado en la memoria - en general con 32 bits de precisi칩n - veremos que los requerimientos de hardware para ejecutar estos modelos no son para nada triviales, tanto en la memoria como en el poder de computo para calcular las predicciones.\n",
    "\n",
    "Esta realidad impide que tales modelos sean deplegados en dispositivos m치s modestos, como celulares o dispositivos en el Edge, donde las capacidades de c칩mputo son m치s limitadas. Podemos imaginar f치cilmente miles de escenarios donde disponer de modelos basados en *deep learning* con las capacidades que conocemos termina siendo muy atractivo. Por ejemplo, escenarios desconectados, escenarios donde se requieren bajas latencias de comunicaci칩n o incluso escenarios donde, por cuestiones de privacidad, los datos no pueden abandonar el dispositivo.\n",
    "\n",
    "Por ejemplo, representar los par치metros utilizando una precisi칩n de 8 bits representa una reducci칩n de 4X la comparado con una precisi칩n de 32 bits. Si ademas sumamos que podr칤amos representar estos paramteros utilizando enteros en lugar de n칰meros en punto flotante - cuya multiplicaci칩n es mucho mas econ칩mica computacionalmente - entonces tendremos ganancias en performance, tanto en la cantidad de memoria requerida para ejecutar el modelo como en los requerimiento de procesamiento. Esto no solo tiene repercusiones en velocidad sino tambi칠n en costos e impacto ambiental. Seg칰n [Dally, 2015](https://media.nips.cc/Conferences/2015/tutorialslides/Dally-NIPS-Tutorial-2015.pdf), en terminos energ칠ticos, estas optimizaciones representan una mejor de 30x en las operaciones de suma y 18.5x en las operaciones de multiplicaci칩n, cuando se las compara con las operaciones en punto flotante de 32 bits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쮸 qu칠 costo?\n",
    "-------------\n",
    "Quantization es una t칠cnica que nos permite reducir la precisi칩n con la que se representan los par치metros de un modelo, por ejemplo, utilizando una precisi칩n de 8 bits. Reducir la precisi칩n con la que se almacenan los par치metros reduce los rangos y los valores que pueden tomar los par치metros, incurriendo as칤 en una perdida en la performance del modelo original. Sin embargo, la pregunta es 쯤ue tanta precisi칩n estamos dispuestos a perder a cambio de un modelo con requerimientos de procesamiento mucho m치s relajados?\n",
    "\n",
    "En muchos casos, tomar un modelo entrenado en funto flotante de 32 bits (FP32) y transformarlo a enteros de 8 bits (INT8) puede resultar en una perdida de performance peque침a, sin embargo, la aplicaci칩n del problema dictar치 si estas perdidas son aceptables o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M칠todos\n",
    "-------\n",
    "\n",
    "En lo que respecta al formato n칰mero que se utilizar치 para almacenar los par치metros, existen 2 conceptos importantes a tener en cuenta: el rango y la precisi칩n. El rango determina los valores m치ximos y m칤nimos que se pueden almacenar, mientras que la precisi칩n indica la precisi칩n con "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0c26a04c01997af4d3a54c44ba2029caf4208eaf3de13f3aa81bddca06af044"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('sphinx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
