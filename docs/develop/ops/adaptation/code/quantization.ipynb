{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition warning\">PRECAUCIÓN 😱: El tema presentado en esta sección está clasificado como avanzado. El entendimiento de este contenido es totalmente opcional.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization es una técnica que consiste en bajar la precisión de punto flotante utilizada por nuestro modelo para que tenga requerimientos menores de memoria cuando es deplegado en producción, aunque manteniendo mayoritariamente la precisión que el modelo obtuvo cuando se entrenó con precisión mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivación\n",
    "----------\n",
    "\n",
    "En los ultimos tiempos - impulsados por los avances en las técnicas de *deep learning* - los modelos de aprendizaje automático con los que trabajamos han adquirido un tamaño considerable. Por ejemplo, modelos de NLP como RoBERTA tienen aproximadamente 125 millones de parametros. Si nos ponemos a pensar que cada parámetros termina siendo un número de punto flotante almacenado en la memoria - en general con 32 bits de precisión - veremos que los requerimientos de hardware para ejecutar estos modelos no son para nada triviales, tanto en la memoria como en el poder de computo para calcular las predicciones.\n",
    "\n",
    "Esta realidad impide que tales modelos sean deplegados en dispositivos más modestos, como celulares o dispositivos en el Edge, donde las capacidades de cómputo son más limitadas. Podemos imaginar fácilmente miles de escenarios donde disponer de modelos basados en *deep learning* con las capacidades que conocemos termina siendo muy atractivo. Por ejemplo, escenarios desconectados, escenarios donde se requieren bajas latencias de comunicación o incluso escenarios donde, por cuestiones de privacidad, los datos no pueden abandonar el dispositivo.\n",
    "\n",
    "Por ejemplo, representar los parámetros utilizando una precisión de 8 bits representa una reducción de 4X la comparado con una precisión de 32 bits. Si ademas sumamos que podríamos representar estos paramteros utilizando enteros en lugar de números en punto flotante - cuya multiplicación es mucho mas económica computacionalmente - entonces tendremos ganancias en performance, tanto en la cantidad de memoria requerida para ejecutar el modelo como en los requerimiento de procesamiento. Esto no solo tiene repercusiones en velocidad sino también en costos e impacto ambiental. Según [Dally, 2015](https://media.nips.cc/Conferences/2015/tutorialslides/Dally-NIPS-Tutorial-2015.pdf), en terminos energéticos, estas optimizaciones representan una mejor de 30x en las operaciones de suma y 18.5x en las operaciones de multiplicación, cuando se las compara con las operaciones en punto flotante de 32 bits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿A qué costo?\n",
    "-------------\n",
    "Quantization es una técnica que nos permite reducir la precisión con la que se representan los parámetros de un modelo, por ejemplo, utilizando una precisión de 8 bits. Reducir la precisión con la que se almacenan los parámetros reduce los rangos y los valores que pueden tomar los parámetros, incurriendo así en una perdida en la performance del modelo original. Sin embargo, la pregunta es ¿que tanta precisión estamos dispuestos a perder a cambio de un modelo con requerimientos de procesamiento mucho más relajados?\n",
    "\n",
    "En muchos casos, tomar un modelo entrenado en funto flotante de 32 bits (FP32) y transformarlo a enteros de 8 bits (INT8) puede resultar en una perdida de performance pequeña, sin embargo, la aplicación del problema dictará si estas perdidas son aceptables o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos\n",
    "-------\n",
    "\n",
    "En lo que respecta al formato número que se utilizará para almacenar los parámetros, existen 2 conceptos importantes a tener en cuenta: el rango y la precisión. El rango determina los valores máximos y mínimos que se pueden almacenar, mientras que la precisión indica la precisión con "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0c26a04c01997af4d3a54c44ba2029caf4208eaf3de13f3aa81bddca06af044"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('sphinx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
