===========
Performance
===========

En algunas organizaciones podemos tener requerimientos de performance espec铆ficos para nuestros modelos. Esto puede ser el caso por ejemplo de un sitio web que debe mostrar las recomendaciones de compras en un determinado lapso de tiempo para millones de usuarios. Una de las form谩s m谩s directas de alcanzar este tipo requerimientos es a trav茅s de :doc:`../deployment/scaling`. En otros casos, puede ser que los requerimientos de performance tengan que ver con que el modelo debe ejecutarse en un dispositivo que tiene limitaciones de consumo de energia, como es un tel茅fono celular. Si estamos dise帽ando un modelo que predice la pr贸xima palabra que el usuario va a escribir, esta predicci贸n debe realizarse antes que el usuario termine de tipearla. Incluso, es importante tener en mente que los recursos que consumen nuestros modelos *tienen un costo*, el cual podr铆a ser no despreciable y por el c煤al las organizaciones deben pagar. 

En cualquier caso, estas restricciones nos llevar谩n a implementar diferentes optimizaciones para mejorar la performance del modelo y su utilizaci贸n de recursos. Revisaremos en esta secci贸n varias t茅cnicas para alcanzar este objetivo. Por supuesto, una opci贸n para que nuestros modelos se ejecuten m谩s r谩pidamente es utilizar modelos m谩s simples. Sin embargo, cuando esto no es un opci贸n, tenemos varias alternativas:

.. note:: El siguiente contenido es opcional. Los m茅todos que se listan aqui son recientes y avanzados, aunque se utilizan cada vez m谩s en modelos de aprendizaje profundo.

Optimizaci贸n de los procesos de datos
-------------------------------------

En algunos sistemas de procesamiento de datos, operaciones que en un principio pueden ser economicas pueden transformarse en prohibitivas cuando se agrega el factor escala. Imagine modelos de aprendizaje autom谩ticos de detecci贸n de anomal铆as en sensores en organizaciones que disponen en el orden de miles o millones de sensores.

Operaciones como la lectura de los datos de ingreso y su posterior escritura podr铆an ser vitales para alcanzar una performance aceptable. Aqu铆, una buena arquiectura de datos puede ser de gran importancia a la hora de ejecutar nuestros modelos de aprendizaje autom谩tico. Por ejemplo, modelos que pueden ser ejecutados en tecnolog铆as como Apache Spark pueden hacer uso del procesamiento en paralelo distribuido y en memoria, lo cual podr铆a alcanzar niveles de procesamiento elevados, tanto al ejecutarse en tiempo real (Spark dispone de la tecnolog铆a structured-streaming) o por lotes.

El analisis de otras alternativas est谩 fuera del alcance de este curso, pero es importante que el lector tenga en mente estos conceptos en el caso de necesitar realizar implementaciones a escala.


Optimizaciones de ejecuci贸n
---------------------------

.. warning:: El tema presentado en esta secci贸n est谩 clasificado como avanzado . El entendimiento de este contenido es totalmente opcional.

Debido a la forma que funciona el lenguaje Python, la ejecuci贸n de un modelo en este lenguaje ser谩 mas lento que si lo hiciera en un lenguaje como C. Otras opciones incluyen la utilizaci贸n de herramientas de optimizaci贸n como ser `NVIDIA TensorRT <https://developer.nvidia.com/tensorrt>`_, un SDK para la implementaci[on de modelos basados en aprendizaje profundo de alta performance.


Optimizaci贸n de modelos
-----------------------

.. warning:: El tema presentado en esta secci贸n est谩 clasificado como avanzado . El entendimiento de este contenido es totalmente opcional.

Existen t茅cnicas espec铆ficas de optimizaci贸n de modelos que buscan optimizar el compute necesario para ejecutar un determinado modelo. En general, estas t茅cnicas involucran cambiar el orden o cantidad de operaciones matem谩ticas que son necesarias para lograr computar la predicci贸n.

Quantization
************
Quantization es una t茅cnica que consiste en bajar la precisi贸n de punto flotante utilizada por nuestro modelo para que tenga requerimientos menores de memoria cuando es deplegado en producci贸n, aunque manteniendo mayoritariamente la precisi贸n del mismo de cuando se entren贸 con precisi贸n de 32-bits. 

Pruning
*******
Pruning es una t茅cnica en la que se eliminan pesos (weights) de una red neuronal (e incluso capas completas) en aquellos casos donde estos pesos no afectan fuertemente las predicciones del modelo. Esto le permite mantener una presici贸n similar al modelo original, aunque con menos costo computacional.

Distillation
************
Distillation es una t茅cnica en la cual se entrena un modelo *estudiante* para que imite otro modelo, el cual es mas grande y potente.
