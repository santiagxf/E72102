{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP9aApBqQJed"
      },
      "source": [
        "Interpretación de modelos tabulares con LIME\n",
        "============================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V8sUvroQJej"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "El análisis de errores es el proceso para identificar, observar y diagnosticar predicciones erróneas de un modelo de aprendizaje automático, ayudandonos a comprender las áreas con fortalezas o debilidades de un modelo. Cuando decimos que \"la precisión del modelo es del 90%\", puede que no sea uniforme en todos los subgrupos de datos o puede haber algunas condiciones en los datos de entrada en las que el modelo falla más. Por lo tanto, es importante someter las métricas a una revisión más profunda para poder mejorarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oATwJtbQJek"
      },
      "source": [
        "## Utilizando el análisis de errores en el problema censo de la UCI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVyBqp_8QJel"
      },
      "source": [
        "> Nota: Este ejemplo fué adaptado de https://github.com/microsoft/responsible-ai-widgets/blob/main/notebooks/erroranalysis-interpretability-dashboard-census.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRmZrmz7QJel"
      },
      "source": [
        "### Instalación\n",
        "\n",
        "Utilizaremos las librerías `interpret-community`, `raiwidgets`. La instalación de estos paquetes requiere de el compilador g++. Si no lo tiene instalado, puede hacerlo desde:\n",
        "\n",
        "```\n",
        "apt install g++\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yj6mHMaQJel"
      },
      "source": [
        "Necesitaremos instalar las librerias `interpret-community`, `raiwidgets` y `lightgbm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeIXxuSaQJem"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/santiagxf/E72102/master/docs/develop/modeling/selection/code/error_analysis.txt \\\n",
        "    --quiet --no-clobber\n",
        "!pip install -r error_analysis.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTLrI09WQJeo"
      },
      "source": [
        "### Sobre el conjunto de datos del censo UCI\n",
        "\n",
        "El conjunto de datos del censo de la UCI es un conjunto de datos en el que cada registro representa a una persona. Cada registro contiene 14 columnas que describen a una una sola persona, de la base de datos del censo de Estados Unidos de 1994. Esto incluye información como la edad, el estado civil y el nivel educativo. La tarea es determinar si una persona tiene un ingreso alto (definido como ganar más de $50 mil al año). Esta tarea, dado el tipo de datos que utiliza, se usa a menudo en el estudio de equidad, en parte debido a los atributos comprensibles del conjunto de datos, incluidos algunos que contienen tipos sensibles como la edad y el género, y en parte también porque comprende una tarea claramente del mundo real."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWyomYxTQJep"
      },
      "source": [
        "Descargamos el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovck-xIsQJeq"
      },
      "outputs": [],
      "source": [
        "!wget https://santiagxf.blob.core.windows.net/public/datasets/uci_census.zip \\\n",
        "    --quiet --no-clobber\n",
        "!mkdir -p datasets/uci_census\n",
        "!unzip -qq uci_census.zip -d datasets/uci_census"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbVjMnWWQJeq"
      },
      "source": [
        "Lo importamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "K3qSdhVHQJeq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv('datasets/uci_census/data/adult-train.csv')\n",
        "test = pd.read_csv('datasets/uci_census/data/adult-test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo-XmRVqQJer"
      },
      "source": [
        "Generaremos 3 conjuntos de datos: entrenamiento, validación y testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "GQLOUnxnQJet"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "validation = test\n",
        "test, _ = train_test_split(test, test_size=0.9, random_state=1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO43QW_RQJeu"
      },
      "source": [
        "### Entrenando un modelo para explorar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBFC-kurQJeu"
      },
      "source": [
        "Preparando nuestros conjuntos de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "QLjauRV-QJeu"
      },
      "outputs": [],
      "source": [
        "X_train = train.drop(['income'], axis=1)\n",
        "y_train = train['income'].to_numpy()\n",
        "X_test = test.drop(['income'], axis=1)\n",
        "y_test = test['income'].to_numpy()\n",
        "X_val = validation.drop(['income'], axis=1)\n",
        "y_val = validation['income'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "s48KMTQ6QJev"
      },
      "outputs": [],
      "source": [
        "classes = train['income'].unique().tolist()\n",
        "features = X_train.columns.values.tolist()\n",
        "categorical_features = X_train.dtypes[X_train.dtypes == 'object'].index.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7650TEgQJev"
      },
      "source": [
        "Realizaremos un pequeño preprocesamiento antes de entrenar el modelo:\n",
        "\n",
        "- Imputaremos los valores faltantes de las caracteristicas numéricas con la media\n",
        "- Imputaremos los valores faltantes de las caracteristicas categóricas con el valor `?`\n",
        "- Escalaremos los valores numericos utilizando un `StandardScaler`\n",
        "- Codificaremos las variables categóricas utilizando `OneHotEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CWmfAUwmQJev"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "import sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "def prepare(X: pd.DataFrame) -> Tuple[np.ndarray, sklearn.compose.ColumnTransformer]:\n",
        "    pipe_cfg = {\n",
        "        'num_cols': [idx for idx, col_type in enumerate(X_train.dtypes) if col_type.name == 'int64'],\n",
        "        'cat_cols': [idx for idx, col_type in enumerate(X_train.dtypes) if col_type.name == 'object'],\n",
        "    }\n",
        "    \n",
        "    num_pipe = Pipeline([\n",
        "        ('num_imputer', SimpleImputer(strategy='median')),\n",
        "        ('num_scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    cat_pipe = Pipeline([\n",
        "        ('cat_imputer', SimpleImputer(strategy='constant', fill_value='?')),\n",
        "        ('cat_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "    ])\n",
        "    \n",
        "    transformations = ColumnTransformer([\n",
        "        ('num_pipe', num_pipe, pipe_cfg['num_cols']),\n",
        "        ('cat_pipe', cat_pipe, pipe_cfg['cat_cols'])\n",
        "    ])\n",
        "    X = transformations.fit_transform(X)\n",
        "    \n",
        "    return X, transformations\n",
        "\n",
        "\n",
        "X_train_transformed, transformations = prepare(X_train)\n",
        "X_test_transformed = transformations.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky6bIkB8QJev"
      },
      "source": [
        "Entrenamos un modelo basado en `lightgbm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "6vGE0K5KQJew"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "clf = LGBMClassifier(n_estimators=1)\n",
        "model = clf.fit(X_train_transformed, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCZQViKQJew"
      },
      "source": [
        "Configuramos un objeto para general las explicaciones del modelo basado en el conjunto de datos en el que se entreno:\n",
        "\n",
        "> **IMPORTANTE:** Note que esta ténica require la creación de un modelo que sea interpretable. Es decir, en lugar de realizar el análisis en el modelo original (el cual podría tener una complejidad arbitraria), el análsis se hace sobre un modelo que pueda ser facilmente interpretable. Para generar este segundo modelo, se utiliza la técnica de Global Model Surrogate la cual consiste en entrenar un modelo **alumno** que trata de **imitar** al modelo **profesor**. Esta técnica claramente no es exacta y no tenemos ninguna garantía de que los errores que comete el modelo **alumno** son los mismos que los que comete el alumno **profesor**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "jL8RZV7OQJew",
        "outputId": "787ed0e7-774f-468c-fced-4578025ede1f"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-bf366acfb78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                            \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                            \u001b[0mmodel_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                            transformations=transformations)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/dataset/decorator.py\u001b[0m in \u001b[0;36minit_wrapper\u001b[0;34m(self, model, initialization_examples, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialization_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitialization_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minit_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/mimic/mimic_explainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, initialization_examples, explainable_model, explainable_model_args, is_function, augment_data, max_num_of_augmentations, explain_subset, features, classes, transformations, allow_all_transformations, shap_values_output, categorical_features, model_task, reset_index, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             self._datamapper, new_initialization_examples = get_datamapper_and_transformed_data(\n\u001b[1;32m    244\u001b[0m                 \u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialization_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 allow_all_transformations=allow_all_transformations)\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;31m# memory optimization to clear all internal references so they can be gc'ed as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0minitialization_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/_internal/raw_explain/raw_explain_utils.py\u001b[0m in \u001b[0;36mget_datamapper_and_transformed_data\u001b[0;34m(examples, transformations, allow_all_transformations)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mdata_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_all_transformations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_all_transformations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_with_datamapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_mapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/_internal/raw_explain/raw_explain_utils.py\u001b[0m in \u001b[0;36mtransform_with_datamapper\u001b[0;34m(x, datamapper)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx_is_dataset_wrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_dataset_with_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtransformed_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx_is_dataset_wrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/_internal/raw_explain/data_mapper.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# pass a single example through the transformations list to build feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_feature_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/_internal/raw_explain/data_mapper.py\u001b[0m in \u001b[0;36m_build_feature_map\u001b[0;34m(self, x, columns)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# run all transform methods in feature mappers so that we have column counts filled in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_feature_mappers_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mnum_output_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_mappers_pipeline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/_internal/raw_explain/data_mapper.py\u001b[0m in \u001b[0;36m_run_feature_mappers_transform\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_mapper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_mappers_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mfeature_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret_community/_internal/raw_explain/data_mapper_utils.py\u001b[0m in \u001b[0;36mextract_column\u001b[0;34m(x, transformer_config)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([0, 2, 4, 10, 11, 12], dtype='int64')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "from interpret_community.common.constants import ShapValuesOutput, ModelTask\n",
        "from interpret.ext.blackbox import MimicExplainer\n",
        "from interpret.ext.glassbox import LGBMExplainableModel\n",
        "\n",
        "\n",
        "explainer = MimicExplainer(model=model,\n",
        "                           initialization_examples=X_train,\n",
        "                           explainable_model=LGBMExplainableModel,\n",
        "                           augment_data=True, \n",
        "                           max_num_of_augmentations=10,\n",
        "                           features=features, \n",
        "                           classes=classes, \n",
        "                           model_task=ModelTask.Classification,\n",
        "                           transformations=transformations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ije61cNgQJex"
      },
      "source": [
        "Generamos las explicaciones del modelo en nuestro conjunto de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkBUgNUcQJex",
        "outputId": "c13fc9d0-11e8-48a2-bf18-b522e824f4e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
          ]
        }
      ],
      "source": [
        "global_explanation = explainer.explain_global(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIiyx6PQJey"
      },
      "source": [
        "Para generar el análisis de errores, deberemos constuir un pipeline donde tengamos el preprocesamiento y el modelo propiamente dicho en un mismo objeto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "GkH3a-DBQJey"
      },
      "outputs": [],
      "source": [
        "model_pipeline = Pipeline(steps=[('preprocessing', transformations),\n",
        "                                 ('model', model)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "YpjZdrHiYEPa"
      },
      "outputs": [],
      "source": [
        "data = transformations.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "HVMrk14BQUM-"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "from interpret.blackbox import LimeTabular\n",
        "\n",
        "explainer = LimeTabular(predict_fn=model.predict_proba, data=data, feature_names=features, discretize_continuous=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "YJfiJDjrTA1V",
        "outputId": "7e94c632-7702-440b-b203-dac8e34f37ba"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-1cb091585cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlime_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LIME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlime_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/interpret/blackbox/lime.py\u001b[0m in \u001b[0;36mexplain_local\u001b[0;34m(self, X, y, name)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfeature_idx_imp_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlime_explanation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_idx_imp_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "lime_local = explainer.explain_local(data[:5], y_val[:5], name='LIME')\n",
        "\n",
        "show(lime_local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPdopfKuornr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "error_analysis.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('sphinx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "c0c26a04c01997af4d3a54c44ba2029caf4208eaf3de13f3aa81bddca06af044"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
